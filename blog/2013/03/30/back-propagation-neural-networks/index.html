
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>反向传播(BP)神经网络 - Bill's Blog</title>
  <meta name="author" content="Bill Xia">

  
  <meta name="description" content="Yesterday is History, Tomorrow a Mystery, Today is a Gift, Thats why it's called the Present！">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/style.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 0px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-34DJ0T23LB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-34DJ0T23LB');
  </script>
  <link href="/atom.xml" rel="alternate" title="Bill's Blog" type="application/atom+xml">
  <script type="text/javascript">
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}

$(document).bind('DOMNodeInserted', function(event) {
  addBlankTargetForLinks();
});
</script>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'G-34DJ0T23LB']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5786957483481554" crossorigin="anonymous"></script>
</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">Bill's Blog</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/blog/tags">Tags</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://www.google.com/" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:ibillxia.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      
<article class="hentry span9" role="article">

  
  <header class="page-header">
    
      <h1 class="entry-title">反向传播(BP)神经网络</h1>
    
    
      <p class="meta">
        









<time datetime="2013-03-30T21:37:00+08:00" pubdate data-updated="true"></time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>前面几篇文章中对神经网络和深度学习进行一些简介，包括神经网络的发展历史、基本概念和常见的几种神经网络以及神经网络的学习方法等，
本文具体来介绍一下一种非常常见的神经网络模型——反向传播(Back Propagation)神经网络。</p>




<h2>1.概述</h2>


<p>BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文
<em><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Learning representations by back-propagating errors</a></em>
值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。
</p>




<p>BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的
输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断
调整网络的权值和阈值，使网络的误差平方和最小。</p>




<!-- more -->




<h2>2.BP网络模型</h2>


<p>一个典型的BP神经网络模型如图1所示。</br>
<center><img src="/images/2013/IMAG2013033001.jpg"></center>
<center>图1 典型的BP神经网络模型</center></p>




<p>BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的
是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，图1中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，
而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出
取值在(0,1)区间）。图1所示的神经网络的输入输出关系如下：</br>
1)输入层与隐层的关系：</br>
<center>$\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})$.</center>
其中$\boldsymbol{x}$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n × m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。</br>
2)隐层与输出层的关系：</br>
<center>$\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})$.</center>
</p>




<h2>3.BP网络的学习方法</h2>


<p>神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。</br>
假定我们有$q$个带label的样本(即输入)$p_{1},p_{2},...,p_{q}$，对应的label(即期望输出Target)为$T_{1},T_{2},...,T_{q}$，神经网络的实际输出
为$a2_{1},a2_{2},...,a2_{q}$，隐层的输出为$a1[.]$那么可以定义误差函数：</br>
<center>$\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2} $.</center>
BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及
误差的反向传播。对于图1中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为：</br>
<center>$\Delta w2_{ki} = - \eta \frac{\partial E}{\partial w2_{ki}} \\
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} \\
= \eta (t_{k}-a2_{k})f_{2}'a1_{i} = \eta \delta_{ki}a1_{i}$.</center>
其中$\eta$为学习速率。同理可得：</br>
<center>$\Delta b2_{ki} = - \eta \frac{\partial E}{\partial b2_{ki}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}}
= \eta (t_{k}-a2_{k})f_{2}' = \eta \delta_{ki}$.</center>
而隐层的权值变化为：</br>
<center>$\Delta w1_{ij} = - \eta \frac{\partial E}{\partial w1_{ij}} 
= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}}
= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}'w2_{ki}f_{1}'p_{j} = \eta \delta_{ij}p_{j}$.</center>
其中，$\delta_{ij} = e_{i}f_{1}', e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$</br>
同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。</br>
这里我们注意到，输出层的误差为$e_{j},j=1..n$，隐层的误差为$e_{i},i=1..m$，其中$e_{i}$可以认为是$e_{j}$的加权组合，由于作用函数的
存在，$e_{j}$的等效作用为$\delta_{ji} = e_{j}f'()$。
</p>




<h2>4.BP网络的设计</h2>


<p>在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。</p>




<p><strong>1.网络的层数</strong></br>
理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络
复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的
运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。
</p>




<p><strong>2.隐层神经元的个数</strong></br>
网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和
训练网络的时间来恒量一个神经网络设计的好坏：</br>
（1）神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。</br>
（2）神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。</br>
由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。
</p>




<p><strong>3.初始权值的选取</strong></br>
一般初始权值是取值在$(-1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略，
其中$r$为输入个数，$s$为第一层神经元个数。
</p>




<p><strong>4.学习速率</strong></br>
学习速率一般选取为$0.01 - 0.8$，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络，
在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在
不同的阶段设置不同大小的学习速率。
</p>




<p><strong>5.期望误差的选取</strong></br>
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同
的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。
</p>




<h2>5.BP网络的局限性</h2>


<p>BP网络具有以下的几个问题：</br>
<strong>(1)需要较长的训练时间</strong>：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。</br>
<strong>(2)完全不能训练</strong>：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。</br>
<strong>(3)局部最小值</strong>：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。
</p>




<h2>6.BP网络的改进</h2>


<p>BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。
动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习
速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，
反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。</p>




<h2>7.BP网络实现异或</h2>


<p>见参考文献[7]或Andrew Ng. 的ML公开课的第8讲。</p>


<p>另外BP算法的讲解及C++实现参见[4]。</p>




<h2>参考文献</h2>


<p>[1]An Introduction to Back-Propagation Neural Networks: http://www.seattlerobotics.org/encoder/nov98/neural.html</br>
[2]Wiki - Backpropagation: http://en.wikipedia.org/wiki/Backpropagation</br>
[3]Chapter 7 The backpropagation algorithm of Neural Networks - A Systematic Introduction by Raúl Rojas: http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</br>
[4]Back-propagation Neural Net - C++ 实现: http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net</br>
[5]《Visual C++数字图像模式识别技术及工程实践》(第3章)，求实科技 张宏林</br>
[6]《Matlab神经网络设置及应用》(第5章)，周品，清华大学出版社
</p>

</div>
<div id="copyleft" class="well" >
	<span>Original Link: <a rel="full-article" href="">http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/</a></br>
	Attribution - NON-Commercial - ShareAlike - Copyright &copy; <a href="http://about.me/ibillxia">Bill Xia</a> <span>
</div>


  <footer>
    <p class="meta">
      
<span class="byline author vcard">Posted by <span class="fn"><a href="http://about.me/ibillxia">Bill Xia</a></span></span>

      









<time datetime="2013-03-30T21:37:00+08:00" pubdate data-updated="true"></time>
		
		
		
		  <span class="categories">Posted in <a class='category' href='/blog/categories/prml/'>PRML</a></span>
		
		
		  <span class="tags">Tagged with <a class='tag' href='/blog/tags/bpnn/'>BPNN</a>, <a class='tag' href='/blog/tags/ti-du-xia-jiang/'>梯度下降</a>, <a class='tag' href='/blog/tags/shen-jing-wang-luo/'>神经网络</a></span>
		
    </p>
    
      
         <!-- JiaThis Button BEGIN --><div class="jiathis_style_32x32">	<a class="jiathis_button_weixin"></a>	<a class="jiathis_button_qzone"></a>	<a class="jiathis_button_douban"></a>	<a class="jiathis_button_tsina"></a>	<a class="jiathis_button_renren"></a>	<a class="jiathis_button_tqq"></a>	<a class="jiathis_button_fb"></a>	<a class="jiathis_button_twitter"></a>	<a class="jiathis_button_tumblr"></a>	<a class="jiathis_button_googleplus"></a>	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>	<a class="jiathis_counter_style"></a></div><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1362901762548695" charset="utf-8"></script><!-- JiaThis Button END -->
      
    
	</br>
	 <ul class="pager">
      
      <li class="previous"><a class="basic-alignment left"
        href="/blog/2013/03/27/learning-process-of-neural-networks/" title="Previous Post:
        神经网络的学习方法概述">&laquo; 神经网络的学习方法概述</a></li>
      
      <li><a href="/blog/archives">Blog Archives</a></li>
      
      <li class="next"><a class="basic-alignment right" href="/blog/2013/04/04/why-do-Fourier-transformation/"
        title="Next Post: 为什么要进行傅立叶变换">为什么要进行傅立叶变换
        &raquo;</a></li>
      
    </ul>
    
    <section>
      <h1>Comments</h1>
      <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </section>
    
   
  </footer>
</article>

<aside class="sidebar-nav span3">
  
    
  
</aside>


    </div>
  </div>
  <footer role="contentinfo" class="page-footer">
<hr>
<p>
  Copyright &copy; 2009 - 2025 - <a href="http://about.me/ibillxia">Bill Xia</a> -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> - Theme by <a href="https://github.com/bkutil/bootstrap-theme">bootstrap-theme</a> </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ibillxia';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/';
        var disqus_url = 'http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
