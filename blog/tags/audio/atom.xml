<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Audio | Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/blog/tags/audio/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2024-10-08T22:34:18+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux的OSS和ALSA声音系统简介及其比较]]></title>
    <link href="http://ibillxia.github.io/blog/2013/09/08/brief-introduction-of-alsa-and-oss-and-its-comparision/"/>
    <updated>2013-09-08T19:46:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/09/08/brief-introduction-of-alsa-and-oss-and-its-comparision</id>
    <content type="html"><![CDATA[<h2>概述</h2>


<p>昨天想在Ubuntu上用一下HTK工具包来绘制语音信号的频谱图和提取MFCC的结果，但由于前段时间把Ubuntu升级到13.04，系统的声卡驱动
是ALSA（Advanced Linux Sound Architecture，高级Linux声音体系），而不是HTK中所使用的OSS（Open Sound System，开放声音系统）。
网上查阅了大半天，按照 http://forum.ubuntu.org.cn/viewtopic.php?t=316792 中提供的方法用OSS4来替换ALSA，结果OSS4没替换成功，
而原来的ALSA也不好使了，真坑爹啊！到现在还没办法完全复原，现在只能通过alsamixer来设置音量了，系统的音量设置根本无法用，而且
声音设置中的输入设备和输出设备都是空的。（现在将系统升级到13.10版，系统的音量设置可以用了，哈哈）捣鼓了半天也没还原回来唉，
整个人都快崩溃了，都是由于对Linux不熟悉才被虐至如此地步，得恶补啊！！！下面本文就主要介绍一下OSS和ALSA，并将二者进行比较。</p>




<p>在介绍OSS和ALSA之前，先介绍一下音频设备的一些基础知识。</br>
数字音频设备，有时也称codec，PCM，DSP，ADC/DAC设备，用来播放或录制数字化的声音。它的指标主要有：采样速率（电话为8K，DVD为96K）、
channel数目（单声道，立体声）、采样分辨率（8-bit，16-bit）等。</br>
mixer（混频器）：用来控制多个输入、输出的音量，也控制输入（microphone，line-in，CD）之间的切换。</br>
synthesizer（合成器）：通过一些预先定义好的波形来合成声音，有时用在游戏中声音效果的产生。</br>
MIDI接口：MIDI接口是为了连接舞台上的synthesizer、键盘、道具、灯光控制器的一种串行接口。</p>




<!--more-->




<h2>OSS开放声音系统简介</h2>


<p>Open Sound System是一个类Unix和POSIX兼容系统上一个可选的声音架构。OSSv3是Linux下原始的声音系统并集成在内核里，但是OSSv4
在2002年OSS成为商业软件时它地位被ALSA所取代。OSSv4在2007年又成为了开源软件，4Front Technologies以GPL协议发布了它的源码。</p>




<p>OSS（Open Sound System）是unix平台上一个统一的音频接口。以前，每个Unix厂商都会提供一个自己专有的API，用来处理音频。这就
意味着为一种Unix平台编写的音频处理应用程序，在移植到另外一种Unix平台上时，必须要重写。不仅如此，在一种平台上具备的功能，
可能在另外一个平台上无法实现。但是，OSS出现以后情况就大不一样了，只要音频处理应用程序按照OSS的API来编写，那么在移植到另外
一个平台时，只需要重新编译即可。因此，OSS提供了源代码级的可移植性。</p>




<p>同时，很多的Unix工作站中，只能提供录音与放音的功能。有了OSS后，给这些工作站带来了MIDI功能，加上音频流、语音识别/生成、
计算机电话（CT）、JAVA以及其它的多媒体技术，在Unix工作站中，同样可以享受到同Windows、Macintosh环境一样的音频世界。另外，
OSS还提供了与视频和动画播放同步的音频能力，这对在Unix中实现动画、游戏提供了帮助。</p>




<p>在Unix系统中，所有的设备都被统一成文件，通过对文件的访问方式（首先open，然后read/write，同时可以使用ioctl读取/设置参数，
最后close）来访问设备.在OSS中，主要有以下的几种设备文件：</br>
/dev/mixer：访问声卡中内置的mixer，调整音量大小，选择音源。</br>
/dev/sndstat：测试声卡，执行cat /dev/sndstat会显示声卡驱动的信息。</br>
/dev/dsp、/dev/dspW、/dev/audio：读这个设备就相当于录音，写这个设备就相当于放音。/dev/dsp与/dev/audio之间的区别在于采样的编码
不同，/dev/audio使用μ律编码，/dev/dsp使用8-bit（无符号）线性编码，/dev/dspW使用16-bit（有符号）线形编码。/dev/audio主要是为了
与SunOS兼容，所以尽量不要使用。</br>
/dev/sequencer：访问声卡内置的，或者连接在MIDI接口的synthesizer。</p>




<p>OSS为音频编程提供三种设备，分别是/dev/dsp，/dev/dspW和/dev/audio，用户可以直接使用Unix的命令来放音和录音，命令cat /dev/dsp >xyz
可用来录音，录音的结果放在xyz文件中；命令cat xyz >/dev/dsp播放声音文件xyz。如果通过编程的方式来使用这些设备，那么Unix平台通过
文件系统提供了统一的访问接口。程序员可以通过文件的操作函数直接控制这些设备，这些操作函数包括：open、close、read、write、ioctl等。</p>




<h2>ALSA高级Linux声音系统简介</h2>


<p>高级Linux声音体系（英语：Advanced Linux Sound Architecture，缩写为ALSA）是Linux内核中，为声卡提供的驱动组件，以替代原先的
OSS（开放声音系统）。一部分的目的是支持声卡的自动配置，以及完美的处理系统中的多个声音设备，这些目的大多都已达到。另一个声音
框架JACK使用ALSA提供低延迟的专业级音频编辑和混音能力。</p>




<p>这个项目开始于为1998年Gravis Ultrasound所开发的驱动，它一直作为一个单独的软件包开发，直到2002年他被引进入Linux内核的开发
版本(2.5.4-2.5.5)。从2.6版本开始ALSA成为Linux内核中默认的标准音频驱动程序集，OSS则被标记为废弃。</p>




<p>ALSA由许多声卡的声卡驱动程序组成，同时它也提供一个称为libasound的API库。应用程序开发者应该使用libasound而不是内核中的ALSA接口。
因为libasound提供最高级并且编程方便的编程接口。并且提供一个设备逻辑命名功能，这样开发者甚至不需要知道类似设备文件这样的低层接口。
相反，OSS/Free驱动是在内核系统调用级上编程，它要求开发者提供设备文件名并且利用ioctrl来实现相应的功能。为了向后兼容，ALSA提供内核
模块来模拟OSS，这样之前的许多在OSS基础上开发的应用程序不需要任何改动就可以在ALSA上运行。另外，libaoss库也可以模拟OSS，而它不需要
内核模块。另外，ALSA还包含插件功能，使用插件可以扩展新的声卡驱动，包括完全用软件实现的虚拟声卡。ALSA提供一系列基于命令行的工具集，
比如混音器(mixer)，音频文件播放器(aplay)，以及控制特定声卡特定属性的工具。</p>




<p>ALSA API主要分为以下几种接口：</br>
控制接口：提供灵活的方式管理注册的声卡和对存在的声卡进行查询。</br>
PCM接口：提供管理数字音频的捕捉和回放。</br>
原始MIDI接口: 支持 MIDI (Musical Instrument Digital Interface)，一种标准电子音乐指令集。这些API提供访问声卡上的MIDI总线。
这些原始借口直接工作在 The MIDI事件上，程序员只需要管理协议和时间。</br>
记时接口: 为支持声音的同步事件提供访问声卡上的定时器。</br>
音序器接口：一个比原始MIDI接口高级的MIDI编程和声音同步高层接口。它可以处理很多的MIDI协议和定时器。</br>
混音器接口：控制发送信号和控制声音大小的声卡上的设备。</p>




<p>API库使用逻辑设备名而不是设备文件。设备名字可以是真实的硬件名字也可以是插件名字。硬件名字使用hw:i,j这样的格式。其中i是卡号，
j是这块声卡上的设备号。第一个声音设备是hw:0,0.这个别名默认引用第一块声音设备并且在本文示例中一真会被用到。插件使用另外的唯一名字。
比如plughw:,表示一个插件，这个插件不提供对硬件设备的访问，而是提供像采样率转换这样的软件特性，硬件本身并不支持这样的特性。</p>




<h2>OSS与ALSA的优缺点比较</h2>


<p>ALSA是一个完全开放源代码的音频驱动程序集，除了像OSS那样提供了一组内核驱动程序模块之外，ALSA还专门为简化应用程序的编写提供了
相应的函数库，与OSS提供的基于ioctl的原始编程接口相比，ALSA函数库使用起来要更加方便一些。利用该函数库，开发人员可以方便快捷的
开发出自己的应用程序，细节则留给函数库内部处理。当然ALSA也提供了类似于OSS的系统接口，不过ALSA的开发者建议应用程序开发者使用
音频函数库而不是驱动程序的API。Ubuntu默认使用ALSA作为底层声音驱动，程序则与PulseAudio交互，这是一个很不错的方案。</p>




<p>下面来比较一下OSS和ALSA的优缺点：</br>
<strong>(1)OSS的优点（对用户来说）</strong></br>
在内核空间（kernel space）里面包含了一个透明软件混音器(vmix)。这样多个程序就可以同时使用声音设备而且没有任何问题。</br>
这个混音器可以让你单独调节各个程序的音量。</br>
对某些老声卡有着更好的支持比如创新（Creative）的X-Fi。</br>
声音程序的初始反应时间一般更好。</br>
对使用OSS的应用程序接口（API）的程序有更好的支持，很多程序都支持OSS的API，而不需要ALSA的模拟。</br>

<strong>(2)OSS的优点（对开发者来说）</strong></br>
清晰的API文档，更易于使用。</br>
支持用户空间的声音驱动。</br>
可移植性强，OSS也可以在BSDs和Solaris下运行。</br>
本身可以跨平台，可以更方便移植到新的操作系统。</br>

<strong>(3)ALSA的优点</strong></br>
ALSA对USB音频设备支持更好，而OSS的输出还在试验中，输入还未实现。</br>
ALSA支持蓝牙声音设备。</br>
ALSA支持AC'97和HDAudio dial-up soft-modems (比如Si3055)。</br>
ALSA对MIDI支持得更好，但用OSS你只能通过软件合成器（如timidity和fluidsynth）来使用MIDI。</br>
ALSA对待机支持更好，而用OSS，你需要在待机前使用soundoff来停止OSS驱动，在恢复后使用soundon来启动OSS。</br>
OSS的jack检测目前在某些HDAudio-powered主板上不能正常工作。也就是说在某些型号的主板上，你可能需要在插入耳机的
时候手动关闭外置扬声器。而ALSA没这个问题。
</p>




<h2>参考资料</h2>


<p>[1]Archlinux上介绍OSS的Wiki：https://wiki.archlinux.org/index.php/OSS_%28%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87%29 </br>
[2]Archlinux上介绍ALSA的Wiki：https://wiki.archlinux.org/index.php/Advanced_Linux_Sound_Architecture_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) </br>
[3]OSS--跨平台的音频接口简介: http://www.ibm.com/developerworks/cn/linux/l-ossapi/ </br>
[4]Linux ALSA声卡驱动之一：ALSA架构简介: http://blog.csdn.net/droidphone/article/details/6271122 </br>
[5]Linux ALSA声卡编程简介: http://enmind.blog.163.com/blog/static/164138001201092334620355/</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习及其在语音方面的应用]]></title>
    <link href="http://ibillxia.github.io/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/"/>
    <updated>2013-04-17T22:43:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing</id>
    <content type="html"><![CDATA[<p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="http://ibillxia.github.io/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[人的听觉系统生理结构（2）——中枢部分]]></title>
    <link href="http://ibillxia.github.io/blog/2012/12/21/human-audio-system-neural-parts/"/>
    <updated>2012-12-21T21:55:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/12/21/human-audio-system-neural-parts</id>
    <content type="html"><![CDATA[<h2>0.概述</h2>


<p>大脑中与听觉相关的部分称为听觉中枢，它纵跨脑干、中脑、丘脑的大脑皮层，是感觉系统中最长的中枢通路之一。自下向上，
主要环节包括：蜗神经核、上橄榄核、外侧丘系核、下丘核、丘脑的内侧膝状体、大脑皮层颞叶的听觉皮层等，图1所示为听觉中枢
的传导通路。由中枢系统的多层传导过程，可以很自然的联想到近两年很热门的Deep Learning的机器学习方法。
<center><img src="/images/2012/IMAG2012122106.jpg"></center>
<center>图 1 听觉中枢传导通路</center>
</p>




<h2>1蜗神经核</h2>


<p>听神经纤维全部终止于蜗神经核，每条神经纤维可分为三个分支，分别支配耳蜗核的三个亚核，即背核、后腹核与前腹核。用微电极记录
单细胞电活动的方法证实，每个亚核都有各自的声音频率代表区（或称音调定位组合），高频分布在各亚核的背侧，即耳蜗底部投射在各亚核
的背上部；低频区分布在各亚核的腹侧，即耳蜗顶部投射在各亚核的腹下区。前腹侧核中的神经元主要是类本原神经元，它能够保存听觉神经
纤维中的时间-位置编码；后腹侧核中主要是建立和振荡反应类型的神经元，它们能够保存听觉神经纤维中的发放率-位置编码；背侧核中主要
是休止和累积反应类型的神经元，它们表现为非单调的发放率-强度关系。</p>




<!-- more -->




<h2>2上橄榄核</h2>


<p>由5个亚核组成，即S形段（上橄榄核固有核或外侧上橄榄核）、副核（内侧上橄榄核）、内侧上橄榄周围核、外侧上橄榄周围核及斜方体核。
斜方体核接受双侧来的纤维，但主要是来自对侧的交叉纤维。有关音调定位的结果不尽相同，其一，为每个亚核均有各自的音调定位组合规律；
其二，是S形段与副核主要对1000Hz以下的低频敏感，斜方体核主要对高频纯音敏感。</p>




<h2>3外侧丘系核</h2>


<p>外侧丘系核为散在于外侧丘系纤维束内的细胞，它们除对声音信号起中转作用外，还是声—惊觉反射通路中的一个组成部分，与脊髓运动
神经元有联系。</p>




<h2>4下丘核</h2>


<p>由占据下丘大部分空间的球形主核（中央核）及其周围带所组成，在周围带内有些散在的核团。近代对下丘的研究工作改变了过去认为
它仅仅是听觉运动反射中枢的概念。实验证明，下丘核是重要的一级听觉中枢，在声音频率、强度、时间因素及声源定位的分析中均起一定
的作用。核内存在着音调定位组合的神经原排列规律，位于此核头端的神经原对高频敏感，尾端对2000Hz以下的低频敏感。</p>




<h2>5内侧膝状体核</h2>


<p>大致可分为背侧与腹侧两部分，位于丘脑水平，属于较高位的听觉中枢。单个神经元诱发放电的潜伏期与下丘核类似，不同神经元可能与
声音频率的综合分析有关。有些细胞表现出对声音时间因素进行分析的特性，有些则经证实存在着功能特性化的神经元。</p>




<h2>6大脑皮层听区</h2>


<p>为最高位的听觉中枢，人类的听觉Ⅰ区在颞横回、Brodmann41与42区，电刺激清醒人脑的这一区域可诱发对侧耳幻听。在听Ⅰ区与
听Ⅱ区的音调定位组合正好相反：听Ⅰ区是高频在前、低频在后，二者之间中频依次排列；听Ⅱ区低频在前、高频在后。应当强调的是，
听觉在大脑皮层的投射是双侧，一侧皮层听区接受双侧耳蜗的传入冲动，或者说是一侧耳蜗的传入冲动投射至双侧皮层，然而皮层对双侧
传入冲动的敏感度不同，对侧比较敏感。</p>




<h2>7听觉的形成</h2>


<p>通过对人听觉系统的深入研究，可总结出听觉的形成机理。即外来声波首先通过外耳对声音的收集和放大，经由耳道传输至耳膜；
然后经过中耳的阻抗匹配后传导至内耳的耳蜗，并通过在耳蜗基膜处产生振荡来激发与毛细胞相连的传出神经末梢产生脉冲；最后
通过听觉神经将信号传达到大脑引起听觉。其中声波主要通过以下两种途径来形成听觉：</br>
1)首先外界的声波振动鼓膜；其次鼓膜的运动又传递给中耳的听小骨，听小骨带动卵形窗运动，并引起耳蜗外淋巴液和内淋巴液的振动；
再次这种振动通过刺激耳蜗内的毛细胞而令其兴奋；然后，将这种兴奋信号转换为神经冲动信号；最后由听神经传到大脑皮层的听觉中枢，
形成听觉。</br>
2)声波的另一种传递途经是骨传导，即声音通过耳蜗骨壁和颅骨的振动传到内耳。正是因为听自己说话时包含了骨传导部分的声音传递，
所以与单纯的由鼓膜、听小骨和听神经传递的声音感觉有所差异。</p>




<h2>8参考文献</h2>


<p>[1] 张文娟. 基于听觉仿生的目标声音识别系统研究. [博]. 中科院.</br>
[2] 中国数字科技馆-听觉：http://amuseum.cdstm.cn/AMuseum/perceptive/page_2.htm</br>
[3] 百度百科-听觉中枢：http://baike.baidu.com.cn/view/1142958.htm</br>
[4] 听觉中枢的生理活动：http://www.longrenwang.cn/yixuebaojian/erkezhishi/201211024861.html
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[人的听觉系统生理结构（1）——外周部分]]></title>
    <link href="http://ibillxia.github.io/blog/2012/12/21/human-audio-system-out-part/"/>
    <updated>2012-12-21T21:43:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/12/21/human-audio-system-out-part</id>
    <content type="html"><![CDATA[<h2>概述</h2>


<p>人的听觉系统是一个十分巧妙的音频信号处理器，它具有良好的抗噪声识别能力，它对声音信号的处理能力就来源于其巧妙的生理结构。</p>




<p>听觉系统可分为两大部分，即耳朵和听觉中枢。其中耳朵又分为外耳、中耳、内耳、听神经，听觉中枢则纵跨脑干、中脑、丘脑的大脑皮层，
是感觉系统中最长的中枢通路之一。</br>
<center><img src="/images/2012/IMAG2012122101.jpg"></center>
<center>图 1 双耳听觉系统</center>
</p>




<!-- more -->


<p><strong>耳朵(外围部分)</strong>
耳朵包括整个的听觉周围系统，即外耳、中耳、内耳以及与脑部相连的听神经，如图2所示。
<center><img src="/images/2012/IMAG2012122102.jpg"></center>
<center>图2 人耳结构图</center>
</p>


<p>&lt;2>1.外耳</h2></p>

<p>外耳是位于鼓膜以外的部分，由耳廓和外耳道组成。</p>




<p>耳廓的形状有利于声波能量的聚集、收集声音，还可以判断声源的位置，同时又起到保护鼓膜的作用。耳廓除了耳垂外，其内部由软骨组成，
具有一定的弹性。耳廓的后面平浅微凸，前面有些回旋凹陷，这些凹陷也起到聚焦和反射声波的作用。外耳道前有耳屏，在耳廓对应耳屏处也
有凸起，称为对耳屏。耳屏和对耳屏几乎将外耳道遮蔽，可以防止异物、飞虫进入外耳道。</p>




<p>外耳道是声波传导的通道，一端开口于耳廓中心，一端终止于鼓膜，外耳道形状弯曲，上皮组织有耳毛和耵聍腺分泌，有抗菌和阻止异物
进入的作用，鼓膜在外耳道底可避免外界的直接损伤。外耳道是一个有效的共鸣腔，能使较弱的声波振动得到加强，即实现了声音的放大作用，
并引起鼓膜振动。正常人外耳道的长度约为25mm，根据驻波原理可知，外耳道发生共振时的共振频率的基频为f1=3400Hz，因此外耳道对 3400Hz 
左右的声音有较大的共振作用，这一频率范围的声音最易造成人耳声音损伤和听力缺损。</p>




<h2>2.中耳</h2>


<p>中耳包括鼓膜、鼓室、听小骨，如图3所示。有一个充满空气的空腔——鼓室，鼓室内有3个小听骨：槌骨、砧骨和镫骨，鼓室的外壁为鼓膜。
槌骨有一个长柄，连接到可以移动的鼓膜；砧骨是锤骨和镫骨之间的桥梁；镫骨是人体最小的命名骨。三块骨头的排列，使鼓膜的震动引起的
锤骨震动，再导致砧骨的震动，从而导致镫骨的震动，这个传递过程称为听骨链。推动卵圆窗的镫骨足板，它会导致耳蜗（内耳的一部份）
内的流体运动。</br>
<center><img src="/images/2012/IMAG2012122103.jpg"></center>
<center>图3 中耳结构</center>
</p>




<p>鼓室居颞骨岩部内，前方借咽鼓管与鼻咽相通，后方借乳突窦与乳突小房通连。鼓室可分为上、下、前、后、外侧、内侧6壁，内有
听小骨、韧带、肌、血管和神经等。</br>
听小骨有3块，即锤骨、砧骨和镫骨。</br>>
锤骨：形如鼓锤，有头、柄、外侧突和前突。锤骨头与砧骨体形成砧锤关节；柄附于鼓膜脐，其上端有鼓膜张肌附着；外侧突为鼓膜紧张部
与松弛部分界标志；前突有韧带连于鼓室前壁。</br>
砧骨：形如砧，分为体、长脚和短脚。体与锤骨头形成砧锤关节，长脚与镫骨头形成砧镫关节，短脚以韧带连于鼓室后壁。</br>
镫骨：形似马镫，可分为头、颈、前脚、后脚和底。底借韧带连于前庭窗边缘，并封闭该窗。</p>




<p>运动听小骨的肌：共有2条，即鼓膜张肌和镫骨肌。</br>
鼓膜张肌：位于咽鼓管上方的鼓膜张肌半管内，止于锤骨柄上端，受下颌神经支配。收缩时可向内侧牵拉锤骨柄，使鼓膜内陷以紧张鼓膜。</br>
镫骨肌：位于锥隆起内，止于镫骨颈，由面神经支配。收缩时向后拉镫骨以使镫骨底前部离开前庭窗，减低迷路内压；并解除鼓膜紧张状态，
是鼓膜张肌的拮抗肌。</br>
咽鼓管连通鼻咽部与鼓室，长3.5~4.0cm。其作用是使鼓室内气压与外界大气压相等，以保持鼓膜内、外两面的压力平衡。</p>




<p>
中耳主要具有以下四大作用：</br>
传递声音：声音在外耳到达鼓膜前是靠空气传导，通过中耳到达内耳耳蜗后是在液体中传导。</br>
声压放大：听骨链的 3 块听小骨相互连接组成了鼓膜至内耳卵形窗之间的机械链，构成了一个杠杆系统，砧骨柄与锤骨柄的杠杆力臂之比
为1:1.3，鼓膜的振动面积与卵形窗的振动面积之比约为17:1，声音振动的压强将增大到原来的 17×1.3 倍，即声压增强了22倍。</br>
保护内耳：由于中耳的特殊结构，其还具有保护内耳免受强声损害的作用，这是因为中耳内连接镫骨的一块肌肉和连接鼓膜的一块肌肉当遇到
较响的声音时，能够起到反射作用，令听小骨所传导的信号减弱，其内部还有一块肌肉可以改变镫骨的轴向，起到减弱声音响度进而保护内耳
的作用，这个反应称为减弱反射。减弱反射有一个 50-100ms 的延时，对突然出现的高声不具有保护作用，只能在一定程度上降低低频高强度
的声音对内耳中耳蜗造成的损害。</br>
阻抗匹配：通常情况下，声音通过阻抗较小的空气介质向阻抗较大的液体传导时，大部分能量将被反射而传递不到内耳，因而将会导致整个
声波的传导效率降低。然而，中耳的生理结构正好弥补了这一点，它的机械阻抗匹配功能有效地补偿了能量的反射，提高了能量的传导效率。</br>
中耳在声波的传输过程中起到关键作用，通过研究表明，中耳的传递函数相当于一个高通滤波器，它在很大程度上决定了人耳的低频听阈。
另外，中耳与外界是相通的，但咽鼓管软骨部具有弹性．平时呈闭合状态，在正常状态下，只有需要调节鼓膜内、外侧压力和排泄中耳粘膜
分泌物时，咽鼓管开放；多数为闭合，这是对中耳、内耳的保护。
</p>




<h2>3内耳</h2>


<p>内耳位在耳朵之最深处，为颞骨包围着，包括由骨密质构成的一系列复杂的曲管，称骨迷路，及其内部的形态与骨迷路基本一致的膜性
曲管，叫做膜迷路两部分构成。膜迷路内充以淋巴液，叫做内淋巴，膜迷路与骨迷路之间的间隙内也有淋巴液，叫做外淋巴。内、外淋巴液
互不交通。</br>
内耳从解剖学的角度可分成两个部分：一个叫做耳蜗是听觉器，另一个是包含前庭和半规管的平衡器。因此内耳又叫做平衡听觉器。
支配它的神经叫做平衡听觉神经是为第八对脑神经。</br>
平衡和听觉两个风牛马不相及的东西怎会凑在一起？在解剖学上，发现两者都浸泡在共通的内外淋巴液之中，因此在临床症状上就产生一些
复杂的关系。平衡障碍可能会导致听觉症状，也就是可能会有听力障碍、耳鸣等症状。所以内耳兼有听觉和感受位置变动的双重功能。</p>




<h4>3.1平衡器——前庭</h4>


<p>前庭平衡器可分成两个部分：一部分是左右耳对称，主控制旋转平衡的三半规管。三个半规管相互垂直，三度空间可谓面面俱到，所以
任凭你的身体或头部处于任何姿态，三半规管都可以管得到无任何死角。因此，可以维持任何姿势的平衡。另一部分是椭圆囊和球状囊，
它是控制直线性平衡的，包括地心引力。</p>




<h4>3.2听觉器——耳蜗</h4>


<p>耳蜗位于骨前庭的前内侧，是一个外形呈蜗牛状卷曲的神经体，长约3.5cm，最宽处约0.32cm，呈螺旋状盘旋2.5～2.75圈，如图4所示。</br>
<center><img src="/images/2012/IMAG2012122104.jpg"></center>
<center>图4 耳蜗的结构</center>
</p>




<p>耳蜗由三个内部充满淋巴液的空腔组成，如图5所示。这三个空腔由上到下依次为：前庭阶，内含外淋巴液体；蜗管，内含内淋巴的盲管；
鼓阶，内含外淋巴液体，鼓阶中的外淋巴在耳蜗顶部通过蜗孔与前庭阶中的外淋巴交通。赖斯纳氏膜（前庭膜）分隔前庭阶和蜗管，基底膜
分隔蜗管和鼓阶。听觉转导器官柯蒂氏器坐落于基底膜之上、蜗管内部。听神经的纤维通过基底膜与内毛细胞和外毛细胞形成突触连接，其
细胞体位于在耳蜗中心部的螺旋神经节。</br>
<center><img src="/images/2012/IMAG2012122105.jpg"></center>
<center>图5 耳蜗横截面</center>
</p>




<p>耳蜗的作用是把传到耳蜗的机械振动转变成听神经纤维的神经冲动，并具有机械频率分析器的功能，能够将复杂的声波分解成一系列频率
信号的组合，声音感觉的很多方面都起源于耳蜗的这种机械特性。在换能这一转变过程中，耳蜗基底膜的振动是一个关键因素，它的振动使
位于它上面的毛细胞受到刺激，引起耳蜗内发生各种过渡性的电变化，最后引起位于毛细胞底部的传入神经纤维产生动作电位。</p>




<h4>3.3基底膜的振动和行波理论</h4>


<p>当声波振动通过听骨链到达卵圆窗膜时，压力变化立即传给耳蜗内液体和膜性结构；如果卵圆窗膜内移，前庭膜和基底膜也将下移，最后
是鼓阶的外淋巴压迫圆窗膜外移；相反，当卵圆窗膜外移时，整个耳蜗内结构又作反方向的移动，于是形成振动。可以看出，在正常气传导的
过程中，圆窗膜实际起着缓冲耳蜗内压力变化的作用，是耳蜗内结构发生振动的必要条件。</p>




<p>有人用直接观察的方法，详细记录了声音刺激引起的基底膜振动的情况，这对于了解基底膜振动的形式，以及这种振动在耳蜗接受不同频率
的声音刺激时有何差异，提供了可靠的依据。观察表明，基底膜的振动是以行波（traveling wave）的方式进行的，即内淋巴的振动首先在靠近
卵圆窗处引起基底膜的振动，此波动再以行波的形式沿基底膜向耳蜗的顶部方向传播，就象人在抖动一条绸带时，有行波沿绸带向远端传播一样。
进一步还证明，不同频率的声音引起的行波都从基底膜的底部，即靠近卵圆窗膜处开始，但频率不同时，行波传播的远近和最大行波的出现部位
有所不同，这就是振动频率愈低，行波传播愈远，最大行波振幅出现的部位愈靠近基底膜顶部，而且在行波最大振幅出现后，行波很快消失，
不再传播；相反地，高频率声音引起的基底膜振动，只局限于卵圆窗附近。</p>




<p>不同频率的振动引起基底膜不同形式的行波传播，主要是由基底膜的某些物理性质决定的。基底膜的长度在人约为30mm，较耳蜗管略短，
但宽度在靠近卵圆窗处只有0.04mm，以后逐渐加宽；与此相对应，基底膜上的螺旋器的高度和重量，也随着基底膜的加宽而变大。这些因素
决定了基底膜愈靠近底部，共振频率愈高，愈靠近顶部，共振频率愈低；这就使得低频振动引起的行波在向顶部传播时阻力较小，而高频振动
引起的行波只限局在底部附近。</p>




<p>不同频率的声音引起的不同形式的基底膜的振动，被认为是耳蜗能区分不同声音频率的基础。破坏动物不同部位基底膜的实验和临床上
不同性质耳聋原因的研究，都证明了这一结论，亦即耳蜗底部受损时主要影响高频听力，耳蜗顶部受损时主要影响低频听力。不难理解，
既然每一种振动频率在基底膜上都有一个特定的行波传播范围和最大振幅区，与这些区域有关的毛细胞和听神经纤维就会受到最大的刺激，
这样，来自基底膜不同区域的听神经纤维的神经冲动及其组合形式，传到听觉中枢的不同部位时，就可能引起不同音调（对应于声波的频率
或波长）的感觉。</p>




<p>基底膜的振动是怎样使毛细胞受到刺激的呢？毛细胞顶端的听毛有些埋在盖膜的胶状物中，有些是和盖膜的下面相接触；因盖膜和基底膜
与蜗轴骨板的连接点不在同一水平，故当行波引起基底膜振动时，基底膜的振动轴和盖膜的振动轴不一致，于是两膜之间有一个横向的交错
移动，使听毛受到一个切向力的作用而弯曲。毛细胞听纤毛的弯曲，是耳蜗中由机械能转为电变化的第一步。</p>




<h2>4 位听神经</h2>


<p>位听神经是12对脑神经当中的第8对，同时也称作前庭耳蜗神经。它是支配内耳的脑神经，其中又可分为掌管听力的耳蜗神经、掌管平衡的
前庭神经。它在颞骨之内，自延髓延伸至内听道，与颜面神经位于在相同的位置。</p>




<p>蜗神经的感觉神经元胞体位于内耳蜗轴内的螺旋神经节，为双极神经元，周围突分布于螺旋器的毛细胞，中枢突在内耳边聚成蜗神经，止于
脑干的蜗神经前、后核，传入听觉冲动。前庭神经的感觉神经元胞体位于内耳道底的前庭神经节，是双极神经元，周围突分布于内耳的球囊斑、
椭圆囊斑和壶腹嵴的毛细胞，中枢突聚成前庭神经。止于脑干的前庭核群及小脑，传入平衡觉冲动。</p>




<p>位听神经将内耳中的感觉细胞（毛细胞）的讯息传递到大脑。一方面由耳蜗神经所构成，负责传递听觉的讯息；另一方面则是前庭神经，
传递平衡的讯息。听神经受损会导致耳聋及平衡失调。</p>




<h2>5.参考文献</h2>


<p>[1] 张文娟. 基于听觉仿生的目标声音识别系统研究. [博]. 中科院.</br>
[2] 中国数字科技馆-听觉：http://amuseum.cdstm.cn/AMuseum/perceptive/page_2.htm</br>
[3] 维基百科-耳：http://zh.wikipedia.org/wiki/%E8%80%B3</br>
[4] 百度百科-外耳：http://baike.baidu.com.cn/view/236771.htm</br>
[5] 百度百科-中耳：http://baike.baidu.com.cn/view/395983.htm</br>
[6] 百度百科-内耳：http://baike.baidu.com.cn/view/236772.htm</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MFCC参数提取及Matalab实现]]></title>
    <link href="http://ibillxia.github.io/blog/2012/07/18/MFCC-feature-extraction/"/>
    <updated>2012-07-18T20:10:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/07/18/MFCC-feature-extraction</id>
    <content type="html"><![CDATA[<h2>1.概述</h2>


<p>Mel倒谱系数（Mel-frequency cepstral coefficients，MFCC）是受人的听觉系统研究成果推动而导出的声学特征。
研究发现，当两个音调的频率差小于临界带宽时，人就会把两个音调听成一个(屏蔽效应)。Mel刻度是对这一临界带宽的
度量方法之一, MFCC在语音识别领域应用广泛。本文详细介绍了Mel频率倒谱系数参数的6大提取步骤。</p>




<h2>2.什么是Mel频率倒谱系数？</h2>


<p>Mel频率倒谱系数（Mel Frequency Cepstrum Coefficient）的缩写是MFCC，Mel频率是基于人耳听觉特性提出来的，
它与Hz频率成非线性对应关系。Mel频率倒谱系数(MFCC)则是利用它们之间的这种关系，计算得到的Hz频谱特征。</p>




<p>用录音设备录制一段模拟语音信号后，经由自定的取样频率(如8000 Hz、16000 Hz等)采样后转换(A/D)为数字语音信号。
由于在时域(time domain)上语音信号的波形变化相当快速、不易观察，因此一般都会在频域(frequency domain)上来观察，
其频谱是随着时间而缓慢变化的，因此通常可以假设在一较短时间中，其语音信号的特性是稳定的，通常我们定义这个较短
时间为一帧(frame)，根据人的语音的音调周期值的变化，一般取10~20ms。</p>




<!-- more -->


<h2>3.Mel频率倒谱系数(MFCC)参数的提取步骤</h2>


<h4>(1) 预加重(pre-emphasis)</h4>


<p>将经采样后的数字语音信号s(n)通过一个高通滤波器(high pass filter)：</br>
<center>$H(z)= 1 – a*z^{-1} , 0.9 < a < 1.0$. </center></br>
其中a一般取0.95左右。经过预加重后的信号为：</br>
<center>$s (n)= s(n)– a×s(n-1)$.</center></br>
因为发声过程中声带和嘴唇的效应，使得高频共振峰的振幅低于低频共振峰的振幅，进行预加重的目的就是为了消除声带和
嘴唇的效应，来补偿语音信号的高频部分。</p>




<h4>(2) 分帧(frame blocking)</h4>


<p>一般取10-30ms为一帧，为了避免窗边界对信号的遗漏，因此对帧做偏移时候，要有帧迭(帧与帧之间需要重叠一部分)。
一般取帧长的一半作为帧移，也就是每次位移一帧的二分之一后再取下一帧，这样可以避免帧与帧之间的特性变化太大。</p>




<h4>(3) 计算短时能量(energy)</h4>


<p>短时能量代表着音量的高低，亦即声音振幅的大小，可以根据此能量的值来过滤掉语音信号中的一些细微噪声。当一帧的能量
值低于我们定的门槛值(threshold)时，则将此帧作为静音段(silence)。</p>




<h4>(4) 加窗(hamming window)</h4>


<p>语音在长范围内是不停变动的，没有固定的特性无法做处理，所以将每一帧代入窗函数，窗外的值设定为0，其目的是消除各个
帧两端可能会造成的信号不连续性。常用的窗函数有方窗、汉明窗和汉宁窗等，根据窗函数的频域特性，常采用汉明窗。公式是在
加窗范围内，$w(i)=0.54-0.46*cos(2* \pi * \frac{i}{n-1}), i \in [0,n-1]$。</p>




<h4>(5) 快速傅立叶变换(FFT transform)</h4>


<p>由于语音信号在时域上的变化快速而不稳定，所以通常都将它转换到频域上来观察，此时它的频谱会随着时间作缓慢的变化。
所以通常将加窗后的帧经过FFT (Fast Fourier Transform)求出每帧的频谱参数。</p>




<h4>(6) 三角形带通滤波器(triangular band-pass filter)</h4>


<p>将每帧的频谱参数通过一组N个三角形带通滤波器(N一般为20~30个)所组成的梅尔刻度滤波器，将每个频带的输出取对数，求出每一个
输出的对数能量(log energy)，k = 1,2,… N。 再将此N个参数进行余弦变换(cosine transform)求出L阶的Mel-scale cepstrum参数。</p>




<h2>4.Matlab程序实现</h2>


<pre><code class="matlab">function r = mfcc(s, fs)
% MFCC参数提取
% Reference: 论文《MFCC和LPCC特征参数在说话人识别中的研究》
%
% Inputs: s  contains the signal to analize
%         fs is the sampling rate of the signal
%
% Output: r contains the transformed signal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
n = 256;  % 帧长
m = 100;  % 帧移
l = length(s);  % 信号总长度
nbFrame = floor((l - n) / m) + 1;  % 信号总帧数
for i = 1:n
    for j = 1:nbFrame
        M(i, j) = s(((j - 1) * m) + i);  % 分帧
    end
end
h = hamming(n);  % Hamming窗w = 0.54 - 0.46*cos(2*pi*x);
M2 = diag(h) * M;  % 对M加窗，形成对角矩阵M2
for i = 1:nbFrame
frame(:,i) = fft(M2(:, i));   % 进行快速傅里叶变换，将其转换到频域上
end
t = n / 2;
% tmax = l / fs;
m = melfb(20, n, fs);  % 调用20阶MEL滤波器组进行滤波
n2 = 1 + floor(t);
z = m * abs(frame(1:n2, :  )).^2;  % 取前n2行帧列向量的平方
r = dct(log(z));  % 取对数后进行反余弦变换
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function m = melfb(p, n, fs)
% MELFB  Determine matrix for a mel-spaced filterbank
%
% Inputs:       p   number of filters in filterbank
%               n   length of fft
%               fs  sample rate in Hz
%
% Outputs:      x   a (sparse) matrix containing the filterbank amplitudes
%                   size(x) = [p, 1+floor(n/2)]
%
% Usage:        For example, to compute the mel-scale spectrum of a
%               colum-vector signal s, with length n and sample rate fs:
%               f = fft(s);
%               m = melfb(p, n, fs);
%               n2 = 1 + floor(n/2);
%               z = m * abs(f(1:n2)).^2;
%
%               z would contain p samples of the desired mel-scale spectrum
%
%               To plot filterbanks e.g.:
%               plot(linspace(0, (12500/2), 129), melfb(20, 256, 12500)'),
%               title('Mel-spaced filterbank'), xlabel('Frequency (Hz)');
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
f0 = 700 / fs;
fn2 = floor(n/2);
lr = log(1 + 0.5/f0) / (p+1);
% convert to fft bin numbers with 0 for DC term
bl = n * (f0 * (exp([0 1 p p+1] * lr) - 1));
b1 = floor(bl(1)) + 1;
b2 = ceil(bl(2));
b3 = floor(bl(3));
b4 = min(fn2, ceil(bl(4))) - 1;
pf = log(1 + (b1:b4)/n/f0) / lr;
fp = floor(pf);
pm = pf - fp;
r = [fp(b2:b4) 1+fp(1:b3)];
c = [b2:b4 1:b3] + 1;
v = 2 * [1-pm(b2:b4) pm(1:b3)];
m = sparse(r, c, v, p, 1+fn2);
</code></pre>



<h2>参考文献</h2>


<p>[1] <a href="http://www.semxi.com/TechnologyDetail.aspx?nID=27">http://www.semxi.com/TechnologyDetail.aspx?nID=27</a> </br>
[2] MFCC和LPCC特征参数在说话人识别中的研究</p>



]]></content>
  </entry>
  
</feed>