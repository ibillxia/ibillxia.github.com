<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: 感知机 | Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/blog/tags/gan-zhi-ji/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2025-05-17T17:42:37+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[神经网络模型分类]]></title>
    <link href="http://ibillxia.github.io/blog/2013/03/24/classes-of-neural-networks/"/>
    <updated>2013-03-24T23:07:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/03/24/classes-of-neural-networks</id>
    <content type="html"><![CDATA[<p>本文主要介绍一下几种不同类型的神经网络模型，主要有前馈神经网络，反馈神经网络，自组织神经网络，随机神经网络</p>




<h2>1.前馈神经网络</h2>


<h4>1)自适应线性神经网络(Adaline)</h4>


<p>自适应线性神经网络（Adaptive Linear，简称Adaline) 是由威德罗（Widrow）和霍夫（Hoff）首先提出的。它与感知器的主要不同之处在于
其神经元有一个线性激活函数，这允许输出可以是任意值，而不仅仅只是像感知器中那样只能取0或1。它采用的是W—H学习法则，也称最小均方差(LMS)
规则对权值进行训练。自适应线性元件的主要用途是线性逼近一个函数式而进行模式联想。</p>




<h4>2)单层感知器</h4>


<p>单层感知器（Perceptron）是由美国计算机科学家罗森布拉特（F.Roseblatt）于1957年提出的。它是一个具有单层神经元的网络，由线性阈值
逻辑单元所组成。它的输入可以是非离散量，而且可以通过学习而得到，这使单层感知器在神经网络研究中有着重要的意义和地位：它提出了自组织、
自学习的思想，对能够解决的问题，有一个收敛的算法，并从数学上给出了严格的证明。</p>


<!-- more -->




<h4>3)多层感知器</h4>


<p>单层感知器由于只有一个神经元，功能单一，只能完成线性决策或实现“与”、“或”、“非”等单一逻辑函数。多层感知器（Multilayer Perceptron）
是在单层感知器的基础上发展起来的，它是一种在输入层与输出层之间含有一层或多层隐含结点的具有正向传播机制的神经网络模型。多层感知器克服了
单层感知器的许多局限，它的性能主要来源于它的每层结点的非线性特性（节点输出函数的非线性特性）。如果每个结点是线性的，那么多层感知器的
功能就和单层感知器一样。</p>




<p>在人工神经网络中，应用最普遍的是多层前馈网络模型。在1986年，Rumelhant和McClelland提出了多层前馈网络的误差反向传播（Error Back Propagation）
学习算法，简称BP算法，这是一种多层网络的逆推学习算法。由此采用BP算法的多层前馈网络也广泛被称为BP网络。</p>




<h2>2.反馈神经网络</h2>


<p>反馈神经网络模型可用一完备的无向图表示。从系统的观点看，反馈神经网络模型是一反馈动力学系统，它具有极复杂的动力学特性。在反馈神经网络模型中，
我们关心的是其稳定性，稳定性是神经网络相联存储性质的体现，可以说稳定就意味着完成回忆。从计算的角度讲，反馈神经网络模型具有比前馈神经网络模型
更强的计算能力，它包括Hopfield神经网络、海明神经网络和双向联想存储器。</p>




<h4>1)Hopfield神经网络</h4>


<p>1982年，美国神经网络学者霍普菲尔德（J.J.Hopfield）提出了反馈型的全连接神经网络，是一种对记忆功能的较好模拟。Hopfield神经网络的结构特点是：
每一个神经元的输出信号通过其它神经元后，反馈到自己的输入端。这种反馈方式有利于通过联想记忆实现最优化，经过分析比较与判断确定最优解决问题的方法。
网络状态的演变是一种非线性动力学系统的行为描述过程，作为一种非线性动力学系统，系统从初始化出发后，系统状态经过演变可能发生如下结果： </br>
a) 渐进稳定形成稳定点，又称为吸引子。</br>
b) 极限环状态。</br>
c) 混沌状态。</br>
d) 发散状态。</br>
发散状态是不希望看到的。对于人工神经网络而言，由于选取网络的变换函数为一个有界函数，因此系统状态不会演变成发散。</p>




<h4>2)海明神经网络(Hamming)</h4>


<p>海明（Hamming）网络由匹配子网和竞争子网组成。匹配子网在学习阶段将若干类别的样本记忆存储在网络的连接权值中；在工作阶段（回忆阶段），
该子网计算输入模式和各个样本模式的匹配程度，并将结果送入竞争子网中，由竞争子网选择出匹配子网中最大的输出。从而，实现了对离散输入模式
进行在海明距离最小意义下的识别和分类。</p>




<h4>3)双向联想存储器(BAM)</h4>


<p>双向联想存储器（BAM）是由日本的Kosko提出的一种神经网络模型，它是ART网络模型的一种简化形式， 是一种异联想存储器。它能存储成对的
模式$(A1，B1)， (A2 ,B2), ⋯,( AN, BN)$。Ai和Bi是不同向量空间中的向量。如果模式A输入到BAM，输出是模式B，且若A与iA最为接近，B就是在BAM所
存储的向量iB。 BAM网络模型中的神经元为非线性单元，每个神经元的作用相当于一个非线性函数，这个函数一般取为S型函数：$y = \frac{1}{1+exp^{-x}}$.
</p>




<h2>3.自组织神经网络</h2>


<h4>1)自适应谐振理论(ART)</h4>


<p><p>自适应谐振理论（adaptive resonance theory，简称ART）的目的是为人类的心理和认知活动建立一个统一的数学理论。1976年，美国学者Carpenter
和Grossberg提出了ART神经网络模型。它是利用生物神经细胞的自兴奋与侧抑制的原理来指导学习，让输入模式通过网络的双向连接权的作用来进行比较
与识别，最后使网络对输入模式产生所谓的谐振，因此来完成对输入模式的记忆，并以同样的方式实现网络的回想。当网络已经存储了一定的内容之后，
则可用它来进行识别。在识别过程中，如果输入是已记忆的或与已记忆的模式十分相似，则网络会把它回想出来。如果是没有记忆的新模式，则在不影响
原有记忆的前提下，把它记忆下来，并用一个没用过的输出层神经元作为这一新模式的分类标志。<p></p>

<p>ART网络主要有三种形式：ART1是处理双极型或二进制数据，即观察向量的每个分量是二值的，只能取0或1；ART2是用于处理连续型模拟信号，
即观察向量的每个分量可取任意实数值，也可用于二进制输入；ART3是分级搜索模型，它兼容前两种结构的功能并将两层神经元网络扩大为任意
多层神经元网络，并在神经元的运行模型中纳入人类神经元生物电—化学反应机制，因而具备了相当强的功能和扩展能力。</p>


<h4>2)自组织映射神经网络模型(SOM)</h4>


<p>在人的感觉通道上一个很重要的组织原理是神经元有序地排列着，并且往往可以反映出所感觉到外界刺激的某些物理特性。如在听觉通道的每一个层次上，
其神经元与神经纤维在结构上的排列与外界刺激的频率关系十分密切，对于某个频率，相应的神经元具有最大的响应，这种听觉通道上的有序排列一直延续到
听觉皮层，尽管许多低层次上的组织是预先排好的，但高层次上的神经组织则是通过学习自组织而形成的。由此生物背景，提出了自组织映射神经网络模型（SOM）。</p>


<h4>3)对流神经网络模型(CPN)</h4>


<p>CPN是由SOM模型和Grossberg外星网络组合而形成的一种神经网络模型。是由美国Hecht-Nielsen和Robert-Nielsen于1987年首先提出来的。一般认为，
这种由两种或多种网络组合而成的新型网络往往具有比原网络模型更强的能力，它能够克服单个网络的缺陷，而且学习时间较短。</p>


<h2>4.随机神经网络</h2>


<h4>1) 模拟退火算法</h4>


<p>在物理学中，对固体物质进行退火处理时，通常先将它加温溶化，使其中的粒子可自由地运动，然后随着物质温度的下降，粒子也形成了低能态的晶格。
若在凝结点附近的温度下降速度足够慢，则固体物质一定会形成最低能量的基态。对于组合优化问题来说，它也有类似的过程，也就是说物理中固体物质的
退火过程与组合优化问题具有相似性。组合优化问题也是在解空间寻求花费函数最小（或最大）的解。</p>


<h4>2) Boltzmann机</h4>


<p>Boltzmann机是由Hinton和Sejnowski提出来的一种统计神经网络模型，是在Hopfield网络基础之上引入了随机性机制而形成的。与Hopfield神经网络不同
的是Boltzmann机具有学习能力，即其权值通过学习来调整，而不是预先设置。Boltzmann机是一种约束满足神经网络模型。</p>


<p>基于模拟退火算法的波尔兹曼机训练的基本思想为：当神经网络中某个与温度对应的参数发生变化时，神经网络的兴奋模式也会如同物理上的热运动那
样发生变化：当温度逐渐下降时，由决定函数判断神经元是否处于兴奋状态。在从高温到低温的退火(annealing)中，能量并不会停留在局部极小值上，
而以最大的概率到达全局最小值。</p>


<h3>推荐资料</h3>


<p>Machine Learning Lecture by Andrew Ng, Stanford University</br>
Lecture VIII: Neural Network - Representation</br>
Lecture IX: Neural Network - Learning</br>
Video courses on Coursera: https://class.coursera.org/ml-2012-002/lecture/index</br>
Lecture homepage in Standford: http://cs229.stanford.edu/</p>


<h3>参考文献</h3>


<p>[1] Simon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)</br>
[2]T-61.3030 <a href="http://www.cis.hut.fi/Opinnot/T-61.3030/schedule2007.shtml">PRINCIPLES OF NEURAL COMPUTING</a> (5 CP)</br>
[3]人工神经网络综述：<a href="http://ishare.iask.sina.com.cn/f/36537774.html">http://ishare.iask.sina.com.cn/f/36537774.html</a></br>
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神经网络简介]]></title>
    <link href="http://ibillxia.github.io/blog/2013/03/20/basics-of-neural-networks/"/>
    <updated>2013-03-20T23:21:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/03/20/basics-of-neural-networks</id>
    <content type="html"><![CDATA[<p>Deep Learning的本质是多层的神经网络，因此在深入学习Deep Learning之前，有必要了解一些神经网络的基本知识。
本文首先对神经网络的发展历史进行简要的介绍，然后给出神经元模型的形式化描述，接着是神经网络模型的定义、特性，
最后是一些最新的进展等。关于神经网络的分类、学习方法、应用场景等将在后续文章中介绍。</p>




<h2>1.发展简史</h2>


<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts建立了神经网络和数学模型，称为MP模型。他们通过MP模型提出
了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。</br>
1945年，Von Neumann在成功的试制了存储程序式电子计算机后，他也对人脑的结构与存储式计算机进行的根本区别的比较，还提出了以简单神经元构成的自再生自动机网络结构。</br>
1949年，心理学家D.O.Heb提出了突触联系强度可变的设想，并据此提出神经元的学习准则——Hebb规则，为神经网络的学习算法奠定了基础。</br>
1958年，F.Rosenblatt提出了感知模型，该模型是由阈值神经元组成的，它试图模拟动物和人的感知和学习能力。</br>
1962年Widrow提出了自适应线性元件，这是一种连续的取值的线性网络，主要用于自适应信号处理和自适应控制。</p>




<!-- more -->


<p>Minkey和Papert从数学上对感知机的功能及其局限性做了深入的分析，于1969年出版了《Perceptron（感知机）》一书，
提出感知机不可能实现复杂的逻辑函数，他们认为感知机的功能是有限的，不能解决如XOR这样的基本问题，而且多层的
网络还不能找到有效的计算方法，进而否定了这一模型。（其实在后来发现，加入隐藏层就可以解决XOR问题。）</br>
虽然冯诺依曼结构和感知机结构大概是一个历史阶段的产物，由于《Perceptron》一书的悲观情绪和冯诺依曼机的快速发展，
神经网络进入了低潮。直到1982年Hopfield提出了HNN模型，他引入了“计算能量函数”的概念，给出了网络稳定性的判据，
推动了人工神经网络技术得以发展。特别是他提出的电子电路的实现为神经计算机的研究奠定的基础。</br>
1986年，Rumelhart及LeCun等学者提出了多层感知器的反向传播算法，克服了当初阻碍感知机继续发展的重要障碍。
与此同时，冯诺依曼机在处理视觉、听觉、联想记忆都方面都体现出了局限性，促使人们开始寻找更加接近人脑的
计算模型，于是又产生了对神经网络研究的热潮。</br>
目前，神经网络的发展非常迅速，从理论上对它的计算能力、对任意连续函数的逼近能力、学习理论以及动态网络的
稳定性分析上都取得了丰硕的成果。特别是在应用上已迅速扩展到许多重要领域，如模式识别与图像处理中的手写体
字符识别，语音识别，人脸识别，基因序列分析，控制及优化，；金融中的股票市场预测，借贷风险管理，信用卡欺骗检测等。
</p>




<h2>2 神经元模型</h2>


<h3>2.1概述</h3>


<p>神经网络的基本组成单元是神经元，在数学上的神经元模型是和在生物学上的神经细胞对应的，也就是说，人工神经网络理论是
用神经元这种抽象的数学模型来描述客观世界的生物细胞的。因此，生物的神经细胞是神经网络理论诞生和形成的物质基础和源泉。
这样，神经元的数学描述就必须以生物神经细胞的客观行为特性为依据。本节在介绍了生物神经元的基本结构的基础上，给出了神经元
的数学模型和形式化表示。</p>




<h3>2.2生物神经元</h3>


<p>生物神经元是大脑处理信息的基本单元，人脑大约由1011个神经元组成，神经元互相连接成神经网络。神经元以细胞体为主体，
由许多向周围延伸的不规则树枝状纤维构成的神经细胞，其形状很像一棵枯树的枝干。主要由细胞体、树突、轴突和突触(Synapse，
又称神经键)组成，如图1所示。</br>
<center><img src="/images/2013/IMAG2013032001.png"></center>
<center>图1. 生物神经元结构图</center></br>
更多关于神经元的生物学解剖，信息的处理与传递方式以及工作特点等内容请参见[4]。</p>




<h3>2.3人工神经元模型</h3>


<p>神经元是神经网络中最基本的信息处理单元，其形式化表示如图2所示。</br>
<center><img src="/images/2013/IMAG2013032002.png"></center>
<center>图2 神经元模型的形式化表示</center></p>




<p>一个典型的神经元模型由以下3个部分组成：</br>
1）突触集（a set ofsynapses）：用权重（weights）来表示。</br>
2）加法器（adder）：将加权后的输入进行求和，即$\sum_{j=1}^{n} w_{kj} x_{j}$</br>
3）激活函数（activation function）：也称为压缩函数（squashing function），作用于神经元的输出，一般是一个非线性函数，常见的有。
另外，很多时候一个神经元还包含一个偏置项bk。
可用用如下的数学表达式来刻画一个神经元：</p>


<center>$u_{k} = \sum_{j=1}^{m} w_{kj} x_{j}$，</center>


<center>$y_{k} = \varphi (u_{k} + b_{k})$.</center>


<p>其中，$u_{k}$表示输入的线性加和，$\varphi (·)$表示激活函数，$y_{k}$表示神经元的输出，$x_{i}$表示输入信号，$w_{ki}$表示权重。
很多时候，为了表示的简单，在输入中加入$x_{0}$项、权重中加入$w_{k0}$项，将偏置$b_{k}$表示为$x_{0} * w_{k0}$。</p>




<h2>3. 神经网络模型及其特性</h2>


<h3>3.1概念</h3>


<p>神经网络（Neural Networks，NN）是由大量的、简单的处理单元（称为神经元）广泛地互相连接而形成的复杂网络系统，它反映了人脑功能的
许多基本特征，是一个高度复杂的非线性动力学习系统。一个典型的神经网络结构如图3所示。这是一个多层的（包含两个隐层L2、L3）、包含两个
输出单元的神经网络拓扑结构。</br>
<center><img src="/images/2013/IMAG2013032003.png"></center>
<center>图3 一个典型的神经网络结构</center></p>




<p>人工神经网络（Artificial Neural Network，ANN）结构和工作机理基本上是以人脑的组织结构（大脑神经元网络）和活动规律为背景的，
它反映了人脑的某些基本特征，但并不是要对人脑部分的真实再现，可以说它是某种抽象、简化或模仿。</p>




<h3>3.2基本特性</h3>


<p>神经网络具有四个基本特征：</p>


<h5>（1）非线性</h5>


<p>非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现
为一种非线性关系。具有阈值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。</p>


<h5>（2）非局限性</h5>


<p>一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、
相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。</p>


<h5>（3）非常定性</h5>


<p>人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身
也在不断变化。经常采用迭代过程描写动力系统的演化过程。</p>


<h5>（4）非凸性</h5>


<p>一个系统的演化方向，在一定条件下将取决于某个特定的状态函数。例如能量函数，它的极值相应于系统比较稳定的状态。非凸性是指这种
函数有多个极值，故系统具有多个较稳定的平衡态，这将导致系统演化的多样性。</p>




<h2>4.近期进展</h2>


<p>最近几年神经网络模型又成了研究热点，这是因为深层神经网络的学习方法取得了突破性的成就。2006年，以Hinton为首的研究人员在深度置信
网络（Deep Belief Networks，DBNs）方面的划时代性的工作，极大的减小了深层神经网络的训练和测试误差，从此深度学习的方法一路所向披靡，
在交通路标识别、字符识别、人脸识别、语音识别等方面的顶级Contest中都取得了最佳效果。</p>




<p>值得一提的是，谷歌的“Google Brain”项目，使用16000个PC机，从YouTube视频中找到的1000万张数字照片作为训练数据集，
用非监督学习的方法建立了一个拥有10亿多条连接的深层神经网络，最后成功的从中识别出猫咪的图片。</p>




<h3>参考文献</h3>


<p>[1] Simon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)</br>
[2]T-61.3030 <a href="http://www.cis.hut.fi/Opinnot/T-61.3030/schedule2007.shtml">PRINCIPLES OF NEURAL COMPUTING</a> (5 CP)</br>
[3] Wiki - Neural network: http://en.wikipedia.org/wiki/Neural_network</br>
[4] 百度百科-神经网络模型：http://baike.baidu.com/view/3406239.htm </br>
[5] 人工神经网络综述：http://ishare.iask.sina.com.cn/f/36537774.html </br>
[6] How bio-inspired deep learning keeps winning competitions：http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions </br>
[7] Google's 'brain simulator': 16,000 computers to identify a cat：http://www.smh.com.au/technology/sci-tech/googles-brain-simulator-16000-computers-to-identify-a-cat-20120626-20zmd.html 
</p>

]]></content>
  </entry>
  
</feed>